{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9048274b-e6b7-4fb3-ac31-5509112a3cfc",
   "metadata": {},
   "source": [
    "# MAPPING FLOODWATER FROM RADAR IMAGERY USING SEMANTIC SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8079c44-c5cf-4344-ac67-c906f848c39c",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214114e0-09c6-4d51-8fe4-fde06aab752b",
   "metadata": {},
   "source": [
    "### Install Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330280c5-c1aa-4ae4-a330-9a490beb4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watermark\n",
    "!pip install pandas-path\n",
    "!pip install -U albumentations\n",
    "!pip install pytorch-lightning\n",
    "!pip install segmentation-models-pytorch\n",
    "\n",
    "# Downgrade pandas to allow compatibility with pytorch_lightning\n",
    "!pip install --upgrade pandas==1.2.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa721e5d-b499-47ec-b42d-868305f984e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "# %matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2193ee50-8ea8-49ab-8d14-9ebda7f7c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %watermark -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670a84a-a343-49c4-b70d-d431461f947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639be0b-f73c-4994-a1bb-8eba5c764a56",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c83bdc-ab4b-4c9d-b1d6-774bc7b0d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This is where our downloaded images and metadata live locally\n",
    "DATA_PATH = Path().cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2d11-5d8a-4481-bfd1-17c517d98358",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(\n",
    "    DATA_PATH / \"flood-training-metadata.csv\", parse_dates=[\"scene_start\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02d2d4-80e0-4d54-874f-8bf7689ccba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051de219-f01c-4615-a1fc-6e9a9a8d25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.flood_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4150d-27ff-49b6-ab18-6e449b79284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca5f6a-c505-4f34-baeb-34c0b1441c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.chip_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7d225-b37b-4672-96e1-f805bbddb90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chips per flood id\n",
    "flood_counts = train_metadata.groupby(\"flood_id\")[\"chip_id\"].nunique()\n",
    "flood_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89712b-680c-4aa9-997b-1db327788dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chips per location\n",
    "location_counts = (\n",
    "    train_metadata.groupby(\"location\")[\"chip_id\"].nunique().sort_values(ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47f74e-f809-48b7-8403-47b15f76cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "location_counts.plot(kind=\"bar\", color=\"lightgray\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Number of Chips\")\n",
    "plt.title(\"Number of Chips by Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70338226-188d-434f-905c-33e4fed5d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Year of capture\n",
    "year = train_metadata.scene_start.dt.year\n",
    "year_counts = train_metadata.groupby(year)[\"flood_id\"].nunique()\n",
    "year_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2730c0-d615-4f41-9e9c-7bc9ac51f086",
   "metadata": {},
   "source": [
    "- Events captured from 2016 to 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56212304-f110-4025-b23c-2517b05ef885",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.groupby(\"flood_id\")[\"scene_start\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2563c-1413-49ad-8bdf-2f3f2e60c633",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce462a92-4746-4cc4-9dec-fd25dc07915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_path import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7528f-4b57-4064-a30a-1479e1f26ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata[\"feature_path\"] = (\n",
    "    str(DATA_PATH / \"train_features\")\n",
    "    / train_metadata.image_id.path.with_suffix(\".tif\").path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9b3b7-940b-4d31-9f44-42617fb10076",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata[\"feature_path\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204b019-7ba1-4d69-89c3-23552b0cb1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata[\"label_path\"] = (\n",
    "    str(DATA_PATH / \"train_labels\")\n",
    "    / train_metadata.chip_id.path.with_suffix(\".tif\").path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00979cc7-f46f-4c00-bf8c-012e2132f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata[\"label_path\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f06c96-b9fb-4d92-9485-8b5ebe8b9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3eab8-607a-4bea-9414-7d3606608a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine an arbitrary image\n",
    "image_path = train_metadata.feature_path[0]\n",
    "with rasterio.open(image_path) as img:\n",
    "    metadata = img.meta\n",
    "    bounds = img.bounds\n",
    "    data = img.read(1)  # read a single band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389f3d4-61e0-4de7-bbbd-ecb44b5a6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d9b85-fef3-4e46-b650-74ff585d7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d0619-395c-42b7-b3a3-289b3d5ce549",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0b650-7609-4956-abba-8eaecfb56fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(image_path) as img:\n",
    "    gdal_mask = img.dataset_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5ab36-680f-4869-b67a-b386fdd5b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(image_path) as img:\n",
    "    numpy_mask = img.read(1, masked=True)\n",
    "numpy_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c77068-b402-4a20-b08a-86c454d567d9",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413057ce-f5a6-4f02-ac9d-dd1aacc48b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f58e7-f22c-430b-a370-a32112b074df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualizing Sentinel-1 images\n",
    "def scale_img(matrix):\n",
    "    \"\"\"\n",
    "    Returns a scaled (H, W, D) image that is visually inspectable.\n",
    "    Image is linearly scaled between min_ and max_value, by channel.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.array): (H, W, D) image to be scaled\n",
    "\n",
    "    Returns:\n",
    "        np.array: Image (H, W, 3) ready for visualization\n",
    "    \"\"\"\n",
    "    # Set min/max values\n",
    "    min_values = np.array([-23, -28, 0.2])\n",
    "    max_values = np.array([0, -5, 1])\n",
    "\n",
    "    # Reshape matrix\n",
    "    w, h, d = matrix.shape\n",
    "    matrix = np.reshape(matrix, [w * h, d]).astype(np.float64)\n",
    "\n",
    "    # Scale by min/max\n",
    "    matrix = (matrix - min_values[None, :]) / (\n",
    "        max_values[None, :] - min_values[None, :]\n",
    "    )\n",
    "    matrix = np.reshape(matrix, [w, h, d])\n",
    "\n",
    "    # Limit values to 0/1 interval\n",
    "    return matrix.clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd94046-3a84-4d0e-a3b7-b55f1b1270fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_false_color_composite(path_vv, path_vh):\n",
    "    \"\"\"\n",
    "    Returns a S1 false color composite for visualization.\n",
    "\n",
    "    Args:\n",
    "        path_vv (str): path to the VV band\n",
    "        path_vh (str): path to the VH band\n",
    "\n",
    "    Returns:\n",
    "        np.array: image (H, W, 3) ready for visualization\n",
    "    \"\"\"\n",
    "    # Read VV/VH bands\n",
    "    with rasterio.open(path_vv) as vv:\n",
    "        vv_img = vv.read(1)\n",
    "    with rasterio.open(path_vh) as vh:\n",
    "        vh_img = vh.read(1)\n",
    "\n",
    "    # Stack arrays along the last dimension\n",
    "    s1_img = np.stack((vv_img, vh_img), axis=-1)\n",
    "\n",
    "    # Create false color composite\n",
    "    img = np.zeros((512, 512, 3), dtype=np.float32)\n",
    "    img[:, :, :2] = s1_img.copy()\n",
    "    img[:, :, 2] = s1_img[:, :, 0] / s1_img[:, :, 1]\n",
    "    \n",
    "    return scale_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121be40-0c8c-4d5c-b8f9-4ebc62a7295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_false_color_composite_(path_vv, path_vh):\n",
    "    \"\"\"\n",
    "    Returns a S1 false color composite for visualization.\n",
    "\n",
    "    Args:\n",
    "        path_vv (str): path to the VV band\n",
    "        path_vh (str): path to the VH band\n",
    "\n",
    "    Returns:\n",
    "        np.array: image (H, W, 3) ready for visualization\n",
    "    \"\"\"\n",
    "    # Read VV/VH bands\n",
    "    with rasterio.open(path_vv) as vv:\n",
    "        vv_img = vv.read(1)\n",
    "    with rasterio.open(path_vh) as vh:\n",
    "        vh_img = vh.read(1)\n",
    "\n",
    "    # Stack arrays along the last dimension\n",
    "    s1_img = np.stack((vv_img, vh_img), axis=-1)\n",
    "\n",
    "    # Create false color composite\n",
    "    img = np.zeros((512, 512, 3), dtype=np.float32)\n",
    "    img[:, :, :2] = s1_img.copy()\n",
    "    img[:, :, 2] = s1_img[:, :, 0] / s1_img[:, :, 1]\n",
    "    \n",
    "    return scale_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc45014-8401-41d1-847a-d7937beb8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_chip(random_state):\n",
    "    \"\"\"\n",
    "    Plots a 3-channel representation of VV/VH polarizations as a single chip (image 1).\n",
    "    Overlays a chip's corresponding water label (image 2).\n",
    "\n",
    "    Args:\n",
    "        random_state (int): random seed used to select a chip\n",
    "\n",
    "    Returns:\n",
    "        plot.show(): chip and labels plotted with pyplot\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(1, 2, figsize=(9, 9))\n",
    "\n",
    "    # Select a random chip from train_metadata\n",
    "    random_chip = train_metadata.chip_id.sample(random_state=random_state).values[0]\n",
    "    chip_df = train_metadata[train_metadata.chip_id == random_chip]\n",
    "\n",
    "    # Extract paths to image files\n",
    "    vv_path = chip_df[chip_df.polarization == \"vv\"].feature_path.values[0]\n",
    "    vh_path = chip_df[chip_df.polarization == \"vh\"].feature_path.values[0]\n",
    "    label_path = chip_df.label_path.values[0]\n",
    "\n",
    "    # Create false color composite\n",
    "    s1_img = create_false_color_composite(vv_path, vh_path)\n",
    "\n",
    "    # Visualize features\n",
    "    ax[0].imshow(s1_img)\n",
    "    ax[0].set_title(\"S1 Chip\", fontsize=14)\n",
    "\n",
    "    # Load water mask\n",
    "    with rasterio.open(label_path) as lp:\n",
    "        lp_img = lp.read(1)\n",
    "\n",
    "    # Mask missing data and 0s for visualization\n",
    "    label = np.ma.masked_where((lp_img == 0) | (lp_img == 255), lp_img)\n",
    "\n",
    "    # Visualize water label\n",
    "    ax[1].imshow(s1_img)\n",
    "    ax[1].imshow(label, cmap=\"cool\", alpha=1)\n",
    "    ax[1].set_title(\"S1 Chip with Water Label\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout(pad=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f35d2-364a-4355-b83f-2cd05fd79b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b404741-388a-4c50-be5e-be865e3c02ef",
   "metadata": {},
   "source": [
    "Very little flood water coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6763b-3d78-424a-a880-e14a6f41923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f9b5b-6e92-4855-b3f2-61af1fe46ae1",
   "metadata": {},
   "source": [
    "High flood water coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160a713-21d3-432b-b61f-4c962a9bda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(98)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74265b2-940b-452b-a1ce-a6d220081626",
   "metadata": {},
   "source": [
    "Fairly little flood water coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21148887-2c51-421d-a1d1-012f50bb0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size\n",
    "examples = [rasterio.open(train_metadata.feature_path[x]) for x in range(5)]\n",
    "for image in examples:\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80be69-e914-4677-97c3-399dcce3eca1",
   "metadata": {},
   "source": [
    "## Training/Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c132d0-7099-455d-82b5-e9cdc108846e",
   "metadata": {},
   "source": [
    "- Individual flood events are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d79d7d-f171-4c0e-9210-08c0a8ec4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3161beb-05fd-4cd0-aec9-ce458e0b463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.flood_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38caa1-46a8-44ec-bcec-feffd0c34b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(9)  # set a seed for reproducibility\n",
    "\n",
    "# Sample 3 random floods for validation set\n",
    "flood_ids = train_metadata.flood_id.unique().tolist()\n",
    "val_flood_ids = random.sample(flood_ids, 3)\n",
    "val_flood_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b2fb1-c0ee-41d8-87cf-ef6e2e0801ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = train_metadata[train_metadata.flood_id.isin(val_flood_ids)]\n",
    "train = train_metadata[~train_metadata.flood_id.isin(val_flood_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df18f09-2982-440a-8982-b819449ab4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shape, train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860c8ac-eb90-4a35-b4d8-806c620484fc",
   "metadata": {},
   "source": [
    "A single input to the model contains both VV and VH bands, indentify input by `chip_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a721a7-9f3a-44f9-8d57-4826c0d6968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_by_chip(image_level_df):\n",
    "    \"\"\"\n",
    "    Returns a chip-level dataframe with pivoted columns\n",
    "    for vv_path and vh_path.\n",
    "    Args:\n",
    "        image_level_df (pd.DataFrame): image-level dataframe\n",
    "    Returns:\n",
    "        chip_level_df (pd.DataFrame): chip-level dataframe\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    for chip, group in image_level_df.groupby(\"chip_id\"):\n",
    "        vv_path = group[group.polarization == \"vv\"][\"feature_path\"].values[0]\n",
    "        vh_path = group[group.polarization == \"vh\"][\"feature_path\"].values[0]\n",
    "        paths.append([chip, vv_path, vh_path])\n",
    "    return pd.DataFrame(paths, columns=[\"chip_id\", \"vv_path\", \"vh_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba733ca-680f-406e-9f56-4e9865fc0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9acf69-1577-46fa-a4ca-61135f6bfdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d58c44-7fbc-4c23-a1e1-fe2aa2acd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.label_path[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c5f0d-19ef-4082-a9c5-d4ec75c3110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from labels\n",
    "val_x = get_paths_by_chip(val)\n",
    "# Drop one label for each id, as it is a duplicate.\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "train_x = get_paths_by_chip(train)\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eaea76-0eae-414b-8164-01484751916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation size\n",
    "# Confirm approx. 1/3 of images are in the validation set\n",
    "len(val_x) / (len(val_x) + len(train_x)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41cbcd5-1655-4720-aaa2-4c92bec800f7",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ebed7-8763-4765-a4aa-3a69166577af",
   "metadata": {},
   "source": [
    "### Custom Dataset (PyTorch)\n",
    "The dataset object returns samples as dictionaries with keys for:\n",
    "- `chip_id`: the chip id\n",
    "- `chip`: a two-band image tensor (VV and VH)\n",
    "- `label`: the label mask, if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f2dcd-45ad-4b9c-8861-c751c8f960ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451338e9-3675-4ca9-a8c5-f5c3cb119c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodDataset(Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "    def __init__(self, x_paths, y_paths=None, transforms=None):\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, ndx):\n",
    "        # Loads a 2-channel image from a chip-level dataframe.\n",
    "        instance = self.data.loc[ndx]\n",
    "        \n",
    "        # Read VV/VH bands\n",
    "        with rasterio.open(instance.vv_path) as vv:\n",
    "            vv_band = vv.read(1)\n",
    "        with rasterio.open(instance.vh_path) as vh:\n",
    "            vh_band = vh.read(1)\n",
    "            \n",
    "        # Stack arrays along the last dimension\n",
    "        chip_x = np.stack((vv_band, vh_band), axis=-1)\n",
    "        \n",
    "        # Scale by min/max\n",
    "        min_norm = -77\n",
    "        max_norm = 26\n",
    "        chip_x = np.clip(chip_x, min_norm, max_norm)\n",
    "        chip_x = (chip_x - min_norm) / (max_norm - min_norm)\n",
    "        \n",
    "        # Apply data augmentations, if any.\n",
    "        if self.transforms:\n",
    "            chip_x = self.transforms(image=chip_x)[\"image\"]\n",
    "        chip_x = np.transpose(chip_x, [2, 0, 1])\n",
    "        \n",
    "        # Prepare sample dictionary\n",
    "        sample = {\"chip_id\": instance.chip_id, \"chip\": chip_x}\n",
    "        \n",
    "        # Load label if available.\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[ndx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                chip_y = lp.read(1)\n",
    "            # Apply data argumentations to label\n",
    "            if self.transforms:\n",
    "                chip_y = self.transforms(image=chip_y)[\"image\"]\n",
    "            sample[\"label\"] = chip_y\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66e9e0-3d7e-4370-a91e-6e9f164f5bca",
   "metadata": {},
   "source": [
    "## Data Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d3611-c0fa-4184-8527-11a073355e88",
   "metadata": {},
   "source": [
    "Counter overfitting by increasing the size of training data by applying a set of data augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72988d-dd61-4d36-9906-f6ddf09acb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73925fd9-91df-4882-97e6-e69af98a9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These transformations will be passed to the model class\n",
    "training_transformations = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.RandomCrop(256, 256),\n",
    "        albumentations.RandomRotate90(),\n",
    "        albumentations.HorizontalFlip(),\n",
    "        albumentations.VerticalFlip(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15d1d2-f2e1-40f0-9fcd-bef6d1aef517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "class XEDiceLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Computes (0.5 * CrossEntropyLoss) + (0.5 * DiceLoss).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.xe = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        temp_true = torch.where((true == 255), 0, true)  # cast 255 to 0 temporarily\n",
    "        xe_loss = self.xe(pred, temp_true)\n",
    "        xe_loss = xe_loss.masked_select(valid_pixel_mask).mean()\n",
    "\n",
    "        # Dice loss\n",
    "        pred = torch.softmax(pred, dim=1)[:, 1]\n",
    "        pred = pred.masked_select(valid_pixel_mask)\n",
    "        true = true.masked_select(valid_pixel_mask)\n",
    "        dice_loss = 1 - (2.0 * torch.sum(pred * true)) / (torch.sum(pred + true) + 1e-7)\n",
    "\n",
    "        return (0.5 * xe_loss) + (0.5 * dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab7ee3-2b5f-455d-8d47-7ecd16b82852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_and_union(pred, true):\n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    return intersection.sum(), union.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a0ca8-5615-4580-8b87-3be8aed12fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0488d008-876c-45fc-9de4-df8bf9e89e1f",
   "metadata": {},
   "source": [
    "## Model\n",
    "A baseline model that takes radar imagery as input and outputs binary masks that indicate which pixels in a scene contain floodwater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e1ee5-4bf0-4e0c-9b82-25c84c3e0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7734760-f886-4490-8423-dad0915fd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf model-outputs\n",
    "# !rm -rf tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fb72a-2b27-474e-8cb0-dafc5191f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class FloodModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "        self.backbone = self.hparams.get(\"backbone\", \"resnet34\")\n",
    "        self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.max_epochs = self.hparams.get(\"max_epochs\", 1000)\n",
    "        self.min_epochs = self.hparams.get(\"min_epochs\", 6)\n",
    "        self.patience = self.hparams.get(\"patience\", 4)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 32)\n",
    "        self.x_train = self.hparams.get(\"x_train\")\n",
    "        self.y_train = self.hparams.get(\"y_train\")\n",
    "        self.x_val = self.hparams.get(\"x_val\")\n",
    "        self.y_val = self.hparams.get(\"y_val\")\n",
    "        self.output_path = self.hparams.get(\"output_path\", \"model-outputs\")\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        self.transform = training_transformations\n",
    "\n",
    "        # Save model artefacts\n",
    "        self.output_path = Path.cwd() / self.output_path\n",
    "        self.output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Track validation IOU.\n",
    "        self.intersection = 0\n",
    "        self.union = 0\n",
    "\n",
    "        # Instantiate datasets, model, and trainer params\n",
    "        self.train_dataset = FloodDataset(\n",
    "            self.x_train, self.y_train, transforms=self.transform\n",
    "        )\n",
    "        self.val_dataset = FloodDataset(self.x_val, self.y_val, transforms=None)\n",
    "        self.model = self._prepare_model()\n",
    "        self.trainer_params = self._get_trainer_params()\n",
    "\n",
    "    ## LightningModule methods\n",
    "    def forward(self, image):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        # Calculate training loss\n",
    "        criterion = XEDiceLoss()\n",
    "        xe_dice_loss = criterion(preds, y)\n",
    "\n",
    "        # Log batch xe_dice_loss\n",
    "        self.log(\n",
    "            \"xe_dice_loss\",\n",
    "            xe_dice_loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return xe_dice_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1\n",
    "\n",
    "        # Calculate validation IOU (global)\n",
    "        intersection, union = intersection_and_union(preds, y)\n",
    "        self.intersection += intersection\n",
    "        self.union += union\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection / union\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define optimizer\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Define scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"max\", factor=0.5, patience=self.patience\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }  # logged value to monitor\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Calculate IOU at end of epoch\n",
    "        epoch_iou = self.intersection / self.union\n",
    "\n",
    "        # Reset metrics before next epoch\n",
    "        self.intersection = 0\n",
    "        self.union = 0\n",
    "\n",
    "        # Log epoch validation IOU\n",
    "        self.log(\"val_loss\", epoch_iou, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return epoch_iou\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        unet_model = smp.Unet(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.weights,\n",
    "            in_channels=2,\n",
    "            classes=2,\n",
    "        )\n",
    "        if self.gpu:\n",
    "            unet_model.cuda()\n",
    "        return unet_model\n",
    "\n",
    "    def _get_trainer_params(self):\n",
    "        # Define callback behavior\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=self.output_path,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"max\",\n",
    "            verbose=True,\n",
    "        )\n",
    "        early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=(self.patience * 3),\n",
    "            mode=\"max\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # Specify where TensorBoard logs will be saved\n",
    "        self.log_path = Path.cwd() / self.hparams.get(\"log_path\", \"tensorboard-logs\")\n",
    "        self.log_path.mkdir(exist_ok=True)\n",
    "        logger = pl.loggers.TensorBoardLogger(self.log_path, name=\"benchmark-model\")\n",
    "\n",
    "        trainer_params = {\n",
    "            \"callbacks\": [checkpoint_callback, early_stop_callback],\n",
    "            \"max_epochs\": self.max_epochs,\n",
    "            \"min_epochs\": self.min_epochs,\n",
    "            \"default_root_dir\": self.output_path,\n",
    "            \"logger\": logger,\n",
    "            \"gpus\": None if not self.gpu else 1,\n",
    "            \"fast_dev_run\": self.hparams.get(\"fast_dev_run\", False),\n",
    "            \"num_sanity_val_steps\": self.hparams.get(\"val_sanity_checks\", 0),\n",
    "        }\n",
    "        return trainer_params\n",
    "\n",
    "    def fit(self):\n",
    "        # Set up and fit Trainer object\n",
    "        self.trainer = pl.Trainer(**self.trainer_params)\n",
    "        self.trainer.fit(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92effd-0190-4c9c-9409-e923978f2d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a4fd9c0-0231-4fba-a86e-75c5efab07c6",
   "metadata": {},
   "source": [
    "## Fit The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943da17e-abcd-4007-b51c-f06e851a5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyper parameters\n",
    "hparams = {\n",
    "    # Required hparams\n",
    "    \"x_train\": train_x,\n",
    "    \"x_val\": val_x,\n",
    "    \"y_train\": train_y,\n",
    "    \"y_val\": val_y,\n",
    "    # Optional hparams\n",
    "    \"backbone\": \"resnet34\",\n",
    "    \"weights\": \"imagenet\",\n",
    "    \"lr\": 1e-3,\n",
    "    \"min_epochs\": 6,\n",
    "    \"max_epochs\": 1000,\n",
    "    \"patience\": 4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"val_sanity_checks\": 0,\n",
    "    \"fast_dev_run\": False,\n",
    "    \"output_path\": \"model-outputs\",\n",
    "    \"log_path\": \"tensorboard_logs\",\n",
    "    \"gpu\": torch.cuda.is_available(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad3dc4-f8f4-4be4-95c5-7947bee82a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_model = FloodModel(hparams=hparams)\n",
    "flood_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e791ed-eacc-4b71-b139-446fea74f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_model.trainer_params[\"callbacks\"][0].best_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108d16b-6bd0-4607-a612-9ec2c274a702",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a670d655-8e6b-4c26-8064-23f207d819e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf benchmark-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161f4da-4b0d-4069-b7f7-ae2c98ece0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission folder\n",
    "submission_path = Path(\"benchmark-pytorch\")\n",
    "submission_path.mkdir(exist_ok=True)\n",
    "submission_assets_path = submission_path / \"assets\"\n",
    "submission_assets_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4ff92-6885-4e91-822c-ebd76a3b2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights.\n",
    "weight_path = submission_assets_path / \"flood_model.pt\"\n",
    "torch.save(flood_model.state_dict(), weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396498bd-7408-4639-b63e-ca5553505eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file benchmark-pytorch/flood_model.py\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import rasterio\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "\n",
    "class FloodModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=None,\n",
    "            in_channels=2,\n",
    "            classes=2,\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def predict(self, vv_path, vh_path):\n",
    "        # Switch on evaluation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Create a 2-channel image\n",
    "        with rasterio.open(vv_path) as vv:\n",
    "            vv_img = vv.read(1)\n",
    "        with rasterio.open(vh_path) as vh:\n",
    "            vh_img = vh.read(1)\n",
    "        x_arr = np.stack([vv_img, vh_img], axis=-1)\n",
    "\n",
    "        # Min-max normalization\n",
    "        min_norm = -77\n",
    "        max_norm = 26\n",
    "        x_arr = np.clip(x_arr, min_norm, max_norm)\n",
    "        x_arr = (x_arr - min_norm) / (max_norm - min_norm)\n",
    "\n",
    "        # Transpose\n",
    "        x_arr = np.transpose(x_arr, [2, 0, 1])\n",
    "        x_arr = np.expand_dims(x_arr, axis=0)\n",
    "\n",
    "        # Perform inference\n",
    "        preds = self.forward(torch.from_numpy(x_arr))\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1\n",
    "        return preds.detach().numpy().squeeze().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13181d6-8771-48ad-8300-888bf8910771",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file benchmark-pytorch/main.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from tifffile import imwrite\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import typer\n",
    "\n",
    "from flood_model import FloodModel\n",
    "\n",
    "\n",
    "ROOT_DIRECTORY = Path(\"/codeexecution\")\n",
    "SUBMISSION_DIRECTORY = ROOT_DIRECTORY / \"submission\"\n",
    "ASSETS_DIRECTORY = ROOT_DIRECTORY / \"assets\"\n",
    "DATA_DIRECTORY = ROOT_DIRECTORY / \"data\"\n",
    "INPUT_IMAGES_DIRECTORY = DATA_DIRECTORY / \"test_features\"\n",
    "\n",
    "# Make sure the smp loader can find our torch assets because we don't have internet!\n",
    "os.environ[\"TORCH_HOME\"] = str(ASSETS_DIRECTORY / \"torch\")\n",
    "\n",
    "\n",
    "def make_prediction(chip_id, model):\n",
    "    \"\"\"\n",
    "    Given a chip_id, read in the vv/vh bands and predict a water mask.\n",
    "\n",
    "    Args:\n",
    "        chip_id (str): test chip id\n",
    "\n",
    "    Returns:\n",
    "        output_prediction (arr): prediction as a numpy array\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting inference.\")\n",
    "    try:\n",
    "        vv_path = INPUT_IMAGES_DIRECTORY / f\"{chip_id}_vv.tif\"\n",
    "        vh_path = INPUT_IMAGES_DIRECTORY / f\"{chip_id}_vh.tif\"\n",
    "        output_prediction = model.predict(vv_path, vh_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"No bands found for {chip_id}. {e}\")\n",
    "        raise\n",
    "    return output_prediction\n",
    "\n",
    "\n",
    "def get_expected_chip_ids():\n",
    "    \"\"\"\n",
    "    Use the test features directory to see which images are expected.\n",
    "    \"\"\"\n",
    "    paths = INPUT_IMAGES_DIRECTORY.glob(\"*.tif\")\n",
    "    # Return one chip id per two bands (VV/VH)\n",
    "    ids = list(sorted(set(path.stem.split(\"_\")[0] for path in paths)))\n",
    "    return ids\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    For each set of two input bands, generate an output file\n",
    "    using the `make_predictions` function.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading model\")\n",
    "    # Explicitly set where we expect smp to load the saved resnet from just to be sure\n",
    "    torch.hub.set_dir(ASSETS_DIRECTORY / \"torch/hub\")\n",
    "    model = FloodModel()\n",
    "    model.load_state_dict(torch.load(ASSETS_DIRECTORY / \"flood_model.pt\"))\n",
    "\n",
    "    logger.info(\"Finding chip IDs\")\n",
    "    chip_ids = get_expected_chip_ids()\n",
    "    if not chip_ids:\n",
    "        typer.echo(\"No input images found!\")\n",
    "        raise typer.Exit(code=1)\n",
    "\n",
    "    logger.info(f\"Found {len(chip_ids)} test chip_ids. Generating predictions.\")\n",
    "    for chip_id in tqdm(chip_ids, miniters=25):\n",
    "        output_path = SUBMISSION_DIRECTORY / f\"{chip_id}.tif\"\n",
    "        output_data = make_prediction(chip_id, model).astype(np.uint8)\n",
    "        imwrite(output_path, output_data, dtype=np.uint8)\n",
    "\n",
    "    logger.success(f\"Inference complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b2547-f5b6-4258-b3a0-74eff6ef28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R ~/.cache/torch benchmark-pytorch/assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b100c-f422-4626-a864-5333143a149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "            \n",
    "list_files('benchmark-pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bdb85-33b4-4c5d-81fe-6f1662535ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf benchmark-pytorch/assets/torch/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a2a13-22d3-4aaa-9f29-852db4ed6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files('benchmark-pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf4a47-b0ec-43fd-8e56-f1a69902ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to avoid including the inference dir itself\n",
    "!cd benchmark-pytorch && zip -r ../submission.zip *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e2ab0-cd09-4b8c-b32d-373206fd67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h submission.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45feda4-3088-438c-b40a-feebca8bf1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
